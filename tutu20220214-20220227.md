---
title: tutu20220214-20220227
tags: 作业
category: /小书匠/日记/2022-02
renderNumberedHeading: true
grammar_cjkRuby: true
---

20220214-20220227
Q1：谈谈你对Kafka容错机制的理解，从如下角度出发：
➢ 如何保证宕机时数据不丢

``` javascript
	只有当消息被写入分区的所有副本时，它才被认为是“已提交”的。生产者可以选择接收不同类型的确认，比如在消息被完全提交时的确认、在消息被写入分区首领时的确认，或者在消息被发送到网络时的确认。只要还有一个副本是活跃的，那么已经提交的信息就不会丢失
```

➢ 多副本冗余的高可用机制
	

``` javascript
	通过集群的多副本机制。每个partition不再只有一个，而是有一个leader和多个replica，生产者根据消息的topic和key值，确定了消息要发往哪个partition之后），会找到partition对应的leader，然后将消息发给leader，leader负责消息的写入，并与其余的replica进行同步。一旦某一个partition的leader挂掉了，那么只需提拔一个replica出来，让它成为leader就ok了，系统依旧可以正常运行
```

➢ 多副本之间数据如何同步

``` javascript
	在Kafka中发生复制时确保partition的预写式日志有序地写到其他节点上。N个replicas中。其中一个replica为leader，其他都为follower，leader处理partition的所有读写请求，与此同时，follower会被动定期地去复制leader上的数据。
	Kafka确保从同步副本列表中选举一个副本为leader，或者说follower追赶leader数据。leader负责维护和跟踪ISR中所有follower滞后的状态。当producer发送一条消息到broker后，leader写入消息并复制到所有follower。消息提交之后才被成功复制到所有的同步副本。Follower只复制按顺序同步Leader的消息，而且只有ISR中所有节点都同步成功的消息才算写入成功，所以消息写入延迟受最慢同步副本的限制。在日志复制协议（Log Replication Algorithm）设计上需要需要快速检测慢副本并把其从ISR中剔除。

➢ acks参数
``` javascript
对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没有必要等ISR中的follower全部接收成功。所以Kafka提供了三种可靠性级别，用户可以根据对可靠性和延迟的要求进行权衡。acks

0： producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没写入磁盘就已经返回，当broker故障时可能丢失数据；
1： producer等待leader的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；
-1（all）：producer等待broker的ack，partition的leader和ISR里的follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成重复数据。（极端情况下也有可能丢数据：ISR中只有一个Leader时，相当于1的情况）。
```
➢ ISR的意义

``` javascript
在Kafka中，每个Partition只有Leader能接收Producer的写入，并向Consumer提供消息消费服务，Follower只复制按顺序同步Leader的消息，而且只有ISR中所有节点都同步成功的消息才算写入成功，所以消息写入延迟受最慢同步副本的限制。在日志复制协议（Log Replication Algorithm）设计上需要需要快速检测慢副本并把其从ISR中剔除。
```

➢ acks=all就代表数据一定不会丢失吗

``` javascript
不是，极端情况，ISR就leader，leader接受message 发送ack后就挂了，数据就丢了。
```

Q2：Kafka文件存储机制

``` javascript
一个partition被切割成多个相同大小的segment
	
	由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引的机制，将每个partition分为多个segment
		xxx.index  offset和物理位移pos
			是索引文件，维护该partition的message对应的offset，对应的物理地址 物理偏移量 字节byte
			是【稀疏】维护   换句话说 不是每个message都被维护到index 
			offset 该partition下的全局  是从0开始 绝对offset
			正是有了这个index文件，才能对任一数据写入和查看拥有O(1)的复杂度
			index文件的粒度可以通过参数log.index.interval.bytes来控制，默认是是每过4096字节记录一条index
		xxx.log  数据
			存储message和对应offset
	segment由log.segment.bytes（默认1,073,741,824‬b（1GB））决定，控制每个segment的大小，也可通过log.segment.ms控制，指定多长时间后日志片段会被关闭。					
	这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。每个partition中的每个消息message 都是有一个连续的序列号来标识 ，用于 partition的唯一标识
segment(log和index文件等)如何命名
	每个 LogSegment 都有一个基准偏移量，用来表示当前 LogSegment 中第一条消息的 offset。偏移量是一个 64位的长整形数，固定是20位数字，长度未达到，用 0 进行填补
		index和log文件以当前segment的第一条消息的offset命名，因此第一个index和log文件文件名称 必然是00000000000000000000开头
		
	
```

Q3：Kafka分区分配策略：Range、RoundRobin策略源码分析梳理清楚partition会被哪个消费者组消费
**RangeAssignor**
	Range是默认策略。Range是对每个Topic而言的（即一个Topic一个Topic分），在进行分区分配时，为了尽可能保证所有consumer均匀的消费分区，会对同一个topic中的partition按照序号排序，并对consumer按照字典顺序排序。

	然后为每个consumer划分固定的分区范围，如果不够平均分配，那么排序靠前的消费者会被多分配分区。具体就是将partition的个数除于consumer线程数来决定每个consumer线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多分配分区。
	但随着topic的增多，那么针对每个topic，消费者序号小（例如0-0）的都将多消费1个分区，topic越多比如为N个，C0-0消费的分区可能会比其他消费者多消费N个分区。可以明显的看到这样的分配并不均匀，如果将类似的情形扩大，有可能会出现部分消费者过载的情况，这就是Range分区策略的一个很明显的弊端

``` javascript
The range assignor works on a per-topic basis. For each topic, we lay out the available partitions in numeric order and the consumers in lexicographic order. We then divide the number of partitions by the total number of consumers to determine the number of partitions to assign to each consumer. If it does not evenly divide, then the first few consumers will have one extra partition.
For example, suppose there are two consumers C0 and C1, two topics t0 and t1, and each topic has 3 partitions, resulting in partitions t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2.
The assignment will be:
C0: [t0p0, t0p1, t1p0, t1p1]
C1: [t0p2, t1p2]
Since the introduction of static membership, we could leverage group.instance.id to make the assignment behavior more sticky. For the above example, after one rolling bounce, group coordinator will attempt to assign new member.id towards consumers, for example C0 -> C3 C1 -> C2.
The assignment could be completely shuffled to:
C3 (was C0): [t0p2, t1p2] (before was [t0p0, t0p1, t1p0, t1p1])
C2 (was C1): [t0p0, t0p1, t1p0, t1p1] (before was [t0p2, t1p2])
The assignment change was caused by the change of member.id relative order, and can be avoided by setting the group.instance.id. Consumers will have individual instance ids I1, I2. As long as 1. Number of members remain the same across generation 2. Static members' identities persist across generation 3. Subscription pattern doesn't change for any member
The assignment will always be:
I0: [t0p0, t0p1, t1p0, t1p1]
I1: [t0p2, t1p2]
@Override
public Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,
                                                Map<String, Subscription> subscriptions) {
    // 得到topic和订阅的消费者集合信息，例如{t1:[c1,c2,c3], t2:[c1,c2,c3,c4]}
    Map<String, List<String>> consumersPerTopic = consumersPerTopic(subscriptions);
    Map<String, List<TopicPartition>> assignment = new HashMap<>();
    // 将consumersPerTopic信息转换为assignment，memberId就是消费者client.id+uuid(kafka在client.id上追加的)
    for (String memberId : subscriptions.keySet())
        assignment.put(memberId, new ArrayList<TopicPartition>());

    // 遍历每个Topic，获取所有的订阅消费者，进行每个topic按订阅消费者分配一次
    for (Map.Entry<String, List<String>> topicEntry : consumersPerTopic.entrySet()) {
        String topic = topicEntry.getKey();
        List<String> consumersForTopic = topicEntry.getValue();

        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
        // 如果Topic没有分区，则调过
        if (numPartitionsForTopic == null)
            continue;

         // 将Topic的订阅者根据字典排序
        Collections.sort(consumersForTopic);
         // 总分区数/订阅者的数量 得到每个订阅者应该分配分区数
        int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();
        // 无法整除的剩余分区数量
        int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();

        List<TopicPartition> partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);
        //遍历所有的消费者
        for (int i = 0, n = consumersForTopic.size(); i < n; i++) {
              //分配到的分区的开始位置
            int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);
            // 分配到的分区数量（整除分配到的分区数量，加上1个无法整除分配到的分区--如果有资格分配到这个分区的话。判断是否有资格分配到这个分区：如果整除后余数为m，那么排序后的消费者集合中前m个消费者都能分配到一个额外的分区）
            int length = numPartitionsPerConsumer + (i + 1 > consumersWithExtraPartition ? 0 : 1);
            //给消费者分配分区
            assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length));
        }
    }
    return assignment;
```



**RoundRobinAssignor**
	RoundRobinAssignor的分配策略是将消费组内订阅的所有Topic的分区及所有消费者进行排序后尽量均衡的分配（RangeAssignor是针对单个Topic的分区进行排序分配的）。如果消费组内，消费者订阅的Topic列表是相同的（每个消费者都订阅了相同的Topic），那么分配结果是尽量均衡的（消费者之间分配到的分区数的差值不会超过1）。如果订阅的Topic列表是不同的，那么分配结果是不保证“尽量均衡”的，因为某些消费者不参与一些Topic的分配，在分配partition轮询所有的消费者时，会跳过那些没有订阅相应partition对应topic的消费者。

``` javascript
The round robin assignor lays out all the available partitions and all the available consumers. It then proceeds to do a round robin assignment from partition to consumer. If the subscriptions of all consumer instances are identical, then the partitions will be uniformly distributed. (i.e., the partition ownership counts will be within a delta of exactly one across all consumers.)
For example, suppose there are two consumers C0 and C1, two topics t0 and t1, and each topic has 3 partitions, resulting in partitions t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2.
The assignment will be:
C0: [t0p0, t0p2, t1p1]
C1: [t0p1, t1p0, t1p2]
When subscriptions differ across consumer instances, the assignment process still considers each consumer instance in round robin fashion but skips over an instance if it is not subscribed to the topic. Unlike the case when subscriptions are identical, this can result in imbalanced assignments. For example, we have three consumers C0, C1, C2, and three topics t0, t1, t2, with 1, 2, and 3 partitions, respectively. Therefore, the partitions are t0p0, t1p0, t1p1, t2p0, t2p1, t2p2. C0 is subscribed to t0; C1 is subscribed to t0, t1; and C2 is subscribed to t0, t1, t2.
That assignment will be:
C0: [t0p0]
C1: [t1p0]
C2: [t1p1, t2p0, t2p1, t2p2]
Since the introduction of static membership, we could leverage group.instance.id to make the assignment behavior more sticky. For example, we have three consumers with assigned member.id C0, C1, C2, two topics t0 and t1, and each topic has 3 partitions, resulting in partitions t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2. We choose to honor the sorted order based on ephemeral member.id.
The assignment will be:
C0: [t0p0, t1p0]
C1: [t0p1, t1p1]
C2: [t0p2, t1p2]
After one rolling bounce, group coordinator will attempt to assign new member.id towards consumers, for example C0 -> C5 C1 -> C3, C2 -> C4.
The assignment could be completely shuffled to:
C3 (was C1): [t0p0, t1p0] (before was [t0p1, t1p1])
C4 (was C2): [t0p1, t1p1] (before was [t0p2, t1p2])
C5 (was C0): [t0p2, t1p2] (before was [t0p0, t1p0])
This issue could be mitigated by the introduction of static membership. Consumers will have individual instance ids I1, I2, I3. As long as 1. Number of members remain the same across generation 2. Static members' identities persist across generation 3. Subscription pattern doesn't change for any member
The assignment will always be:
I0: [t0p0, t1p0]
I1: [t0p1, t1p1]
I2: [t0p2, t1p2]
@Override
public Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,
                                                Map<String, Subscription> subscriptions) {
    Map<String, List<TopicPartition>> assignment = new HashMap<>();
    for (String memberId : subscriptions.keySet())
        assignment.put(memberId, new ArrayList<TopicPartition>());
    // 将消费集合先按字典排序，构建成一个环形迭代器
    CircularIterator<String> assigner = new CircularIterator<>(Utils.sorted(subscriptions.keySet()));
   // 按Topic的名称排序，得到Topic下的所有分区
    for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) {
        final String topic = partition.topic();

		//如果轮询到的消费者没有订阅当前partition的topic，跳过当前消费者
        while (!subscriptions.get(assigner.peek().memberId).topics().contains(topic))
            assigner.next();
        // 给消费者分配分区，并轮询到下一个消费者
        assignment.get(assigner.next().memberId).add(partition);
    }
    return assignment;
}

/**
 * 根据消费者得到订阅的Topic下的所有分区
 * Topic按名称字典升序排序后，得到每个Topic的partitions加到allPartitions
 */
public List<TopicPartition> allPartitionsSorted(Map<String, Integer> partitionsPerTopic,
                                                Map<String, Subscription> subscriptions) {
    SortedSet<String> topics = new TreeSet<>();
    for (Subscription subscription : subscriptions.values())
        topics.addAll(subscription.topics());

    List<TopicPartition> allPartitions = new ArrayList<>();
    for (String topic : topics) {
        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);
        if (numPartitionsForTopic != null)
            allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));
    }
    return allPartitions;
}
```

	AbstractPartitionAssignor
``` javascript
    protected static List<TopicPartition> partitions(String topic, int numPartitions) {
        List<TopicPartition> partitions = new ArrayList<>(numPartitions);
        for (int i = 0; i < numPartitions; i++)
            partitions.add(new TopicPartition(topic, i));
        return partitions;
    }
```

**StickyAssignor**
StickyAssignor分区分配算法，目的是在执行一次新的分配时，能在上一次分配的结果的基础上，尽量少的调整分区分配的变动，节省因分区分配变化带来的开销。Sticky是“粘性的”，可以理解为分配结果是带“粘性的”——每一次分配变更相对上一次分配做最少的变动。其目标有两点：

分区的分配尽量的均衡。
每一次重分配的结果尽量与上一次分配结果保持一致。
当这两个目标发生冲突时，优先保证第一个目标。第一个目标是每个分配算法都尽量尝试去完成的，而第二个目标才真正体现出StickyAssignor特性的。

StickyAssignor算法比较复杂，暂时还没看


Q4：Kafka为什么吞吐量高？从Zero-Copy角度详细阐述
inux操作系统 “零拷贝” 机制使用了sendfile方法， 允许操作系统将数据从Page Cache 直接发送到网络，只需要最后一步的copy操作将数据复制到 NIC 缓冲区， 这样避免重新复制数据 。示意图如下：

                               

 

    通过这种 “零拷贝” 的机制，Page Cache 结合 sendfile 方法，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。

 

当Kafka客户端从服务器读取数据时，如果不使用零拷贝技术，那么大致需要经历这样的一个过程：

操作系统将数据从磁盘上读入到内核空间的读缓冲区中。

应用程序（也就是Kafka）从内核空间的读缓冲区将数据拷贝到用户空间的缓冲区中。

应用程序将数据从用户空间的缓冲区再写回到内核空间的socket缓冲区中。

操作系统将socket缓冲区中的数据拷贝到NIC缓冲区中，然后通过网络发送给客户端。



                                                                                       no zero copy

    从图中可以看到，数据在内核空间和用户空间之间穿梭了两次，那么能否避免这个多余的过程呢？当然可以，Kafka使用了零拷贝技术，也就是直接将数据从内核空间的读缓冲区直接拷贝到内核空间的socket缓冲区，然后再写入到NIC缓冲区，避免了在内核空间和用户空间之间穿梭。



                                                                                            zero copy

     可见，这里的零拷贝并非指一次拷贝都没有，而是避免了在内核空间和用户空间之间的拷贝。如果真是一次拷贝都没有，那么数据发给客户端就没了不是？不过，光是省下了这一步就可以带来性能上的极大提升。

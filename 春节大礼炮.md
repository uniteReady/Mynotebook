---
title: 春节大礼炮
tags: 作业
renderNumberedHeading: true
grammar_cjkRuby: true
---

# 完整的春劫大礼包之一
https://help.aliyun.com/document_detail/127980.html
1）Mock数据 200M
	[9/Jun/2015:01:58:09 +0800]  同一天时间
	ip   随机造
	-   死的
	响应时间 	随机数值类型
	"-"  死的
	请求方法： GET/POST
	URL请求：  3-5个   http://www.ruozedata.com/1.html  2.html  https://...3.html  4.html 5.html
	状态码: 404  505  200  
	请求大小: 随机数值类型
	请求返回大小(bigint):  90%随机数值类型  10%字符串类型(脏数据)
	命中信息： HIT  MISS
	
2）	对这份数据进行清洗，使用RDD的算子，输出成分区格式
	ip ==> province  city  isp  github搜ipparse
	URL ==> protocol: http/https
			domain
			path
	[9/Jun/2015:01:58:09 +0800] ==>  20150609  
		==>/....../day=20150609/....
		hour=01
		time=[9/Jun/2015:01:58:09 +0800]
	2-1) RDD
	2-2) DF
3） 有脏数据，filter脏数据，计数(总条数、错误数、正确数)并存到MySQL中		
==============以上是jar==以下是sh==========
**ETL代码 DS版本 LogETLDS**

``` javascript
package etl

import org.apache.commons.logging.LogFactory
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.{SparkConf, SparkContext}
import utils.{FileUtils, IPUtil, MySQLUtils}

import java.sql.{Connection, PreparedStatement}
import java.text.SimpleDateFormat
import java.util.{Date, Locale}


object LogETLDS {
  def main(args: Array[String]): Unit = {
    println("######开始了#######")
    println("")
    System.setProperty("HADOOP_USER_NAME","root")

    val log = LogFactory.getLog(this.getClass)

    val conf = new SparkConf()
      .setMaster("local[2]")
      .setAppName(this.getClass.getSimpleName)
      //使用Kryo序列化
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

    val spark = SparkSession.builder()
      .config(conf).getOrCreate()
    import spark.implicits._

    val sc = spark.sparkContext



    sc.hadoopConfiguration.set("dfs.client.use.datanode.hostname","true")
    sc.hadoopConfiguration.set("dfs.defaultFS","hdfs://bigdata:9000")

    //    if (args.length != 2){
    //        logger.severe("need 2 args")
    //        System.exit(1)
    //    }
//    val input= "file:///D:\\code\\bigdata2021\\data\\input\\log.txt"
//    val output = "hdfs://bigdata:9000/data/"
//    val ip2regionDbPath = "D:\\code\\bigdata\\ip2region\\ip2region\\data\\ip2region.db"

        val input = args(0)
        val output = args(1)
        val ip2regionDbPath = args(2)
    println("input:"+input)
    println("output:"+output)
    println("ip2regionDbPath:"+ip2regionDbPath)

    FileUtils.deleteOutput(sc.hadoopConfiguration,output)

//    import

    val rdd = sc.textFile(input)
    //dateStr ip proxyIP responsetime referer method url httpcode requestsize responsesize cacheState ua fileType
//[09/Jun/2015:11:28:20 +0800]	61.133.128.0	-	35707	"-"	POST	 http://www.ruozedata.com/1.html	500	526	786	MISS	Mozilla/5.0 (compatible; AhrefsBot/5.0; +http://ahrefs.com/robot/)	text/html
    val SDF = new SimpleDateFormat("[dd/MMM/yyy:HH:mm:ss +0800]", Locale.ENGLISH)
    val SDF2 = new SimpleDateFormat("yyyyMMdd", Locale.ENGLISH)

    val dirtyCount = sc.longAccumulator("dirtyCount")
    val totalCount = sc.longAccumulator("totalCount")

    val rdd02: RDD[UserLog] = rdd.map(line => {

      //进行记录总条数
      totalCount.add(1L)

      val splits = line.split("\t")
      val time = splits(0)
      val date: Date = SDF.parse(time)
      val hours = date.getHours
      var hour = ""
      if (hours < 10) {
        hour = "".concat("0").concat(hours.toString)
      } else {
        hour = "".concat(hours.toString)
      }
      val day: String = SDF2.format(date)

      val ip = splits(1)
      val proxyIP = splits(2)
      val parsedIp = parseIp(ip, ip2regionDbPath)
      val province = parsedIp._1
      val city = parsedIp._2
      val isp = parsedIp._3

      val responsetime = splits(3).toLong
      val referer = splits(4)
      val method = splits(5)

      val url = splits(6)
      val urlSplits = url.split("://")
      val protocol = urlSplits(0)
      val urlSplits2 = urlSplits(1).split("/")
      val domain = urlSplits2(0)
      val path = urlSplits2(1)

      val httpcode = splits(7)
      val requestsize = splits(8).toLong
      val responsesizeStr = splits(9)
      var responsesize = -1L
      //脏数据处理，不赋值，responsesize = -1L
      if (responsesizeStr.contains("脏数据")) {
        dirtyCount.add(1L)
      }
      else {
        responsesize = responsesizeStr.toLong
      }

      val cacheState = splits(10)
      val ua = splits(11)
      val fileType = splits(12)

      UserLog(day, time, hour, ip, proxyIP, responsetime, referer, method, url, httpcode, requestsize, responsesize, cacheState, ua, fileType, province, city, isp, protocol, domain, path)
    }
    )
      //过滤responsesize 的"脏数据"
      .filter(_.responsesize != -1L)

    val df = rdd02.toDF()
//      .filter(_ != null)

    df.coalesce(1)
      .write
      .mode(SaveMode.Overwrite)
      .partitionBy("day")
      .parquet(output)

    println("dirtyCount: " + dirtyCount.value)
    println("totalCount: " + totalCount.value)
    //将etl totalCount等写入mysql
    writeCnt2mysql(totalCount.value,dirtyCount.value)

    sc.stop()
  }

  case class UserLog(day:String, time:String,hour:String,ip:String,proxyIP:String,responseTime:Long,referer:String,method:String,url:String,
                      httpCode:String,requestSize:Long,responsesize:Long,cacheState:String,ua:String,fileType:String,province:String,city:String,isp:String,protocol:String,domain:String,
                      path:String)

  def parseIp(ip:String,ip2regionDbPath:String)  ={
    //995|中国|0|上海|上海市|电信|125682
    val ipMessage = IPUtil.parseIP(ip, ip2regionDbPath)
    val splits = ipMessage.split("\\|")
    //province,city,isp
    (splits(3),splits(4),splits(5))
  }

  def writeCnt2mysql(totalCount:Long,dirtyCount:Long): Unit ={
    var  con : Connection = null;
    var pstmt:PreparedStatement = null
    try {

      con = MySQLUtils.getConnection()
      pstmt = con.prepareStatement("insert  into etl_info (total_cnt,nomal_cnt,error_cnt) values (?,?,?)")
      pstmt.setLong(1,totalCount)
      pstmt.setLong(2,totalCount-dirtyCount)
      pstmt.setLong(3,dirtyCount)

      pstmt.executeUpdate

    } catch {
      case exception: Exception => println(exception)
    } finally {
      if(null != pstmt) pstmt.close()
      MySQLUtils.closeConnection(con)

    }
  }
}

```

**ETL代码 RDD版本 LogETLRDD**
``` javascript
package etl

import org.apache.commons.logging.LogFactory
import org.apache.spark.rdd.RDD
import org.apache.spark.storage.StorageLevel
import org.apache.spark.{SparkConf, SparkContext}
import utils.{FileUtils, IPUtil, MySQLUtils}

import java.sql.{Connection, DriverManager, PreparedStatement, ResultSet}
import java.text.SimpleDateFormat
import java.util.{Date, Locale}


object LogETLRDD {
  def main(args: Array[String]): Unit = {
    println("######开始了#######")
    println("")
//    System.setProperty("HADOOP_USER_NAME","root")

    val log = LogFactory.getLog(this.getClass)
    val conf = new SparkConf()
//      .setMaster("local[2]")
//      .setAppName(this.getClass.getSimpleName)
      //使用Kryo序列化
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    val sc = new SparkContext(conf)

    sc.hadoopConfiguration.set("dfs.client.use.datanode.hostname","true")
    sc.hadoopConfiguration.set("dfs.defaultFS","hdfs://bigdata:9000")

    //    if (args.length != 2){
    //        logger.severe("need 2 args")
    //        System.exit(1)
    //    }
//    val input= "file:///D:\\code\\bigdata2021\\data\\input\\log.txt"
//    val output = "hdfs://bigdata:9000/data/"
//    val ip2regionDbPath = "D:\\code\\bigdata\\ip2region\\ip2region\\data\\ip2region.db"

        val input = args(0)
        val output = args(1)
        val ip2regionDbPath = args(2)
    println("input:"+input)
    println("output:"+output)
    println("ip2regionDbPath:"+ip2regionDbPath)

    FileUtils.deleteOutput(sc.hadoopConfiguration,output)

    val rdd = sc.textFile(input)
    //dateStr ip proxyIP responsetime referer method url httpcode requestsize responsesize cacheState ua fileType
//[09/Jun/2015:11:28:20 +0800]	61.133.128.0	-	35707	"-"	POST	 http://www.ruozedata.com/1.html	500	526	786	MISS	Mozilla/5.0 (compatible; AhrefsBot/5.0; +http://ahrefs.com/robot/)	text/html
    val SDF = new SimpleDateFormat("[dd/MMM/yyy:HH:mm:ss +0800]", Locale.ENGLISH)
    val SDF2 = new SimpleDateFormat("yyyyMMdd", Locale.ENGLISH)

    val dirtyCount = sc.longAccumulator("dirtyCount")
    val totalCount = sc.longAccumulator("totalCount")

    val logRDD: RDD[(String, String)] = rdd.map(line => {
      try {
        val splits = line.split("\t")
        val time = splits(0)
        val date: Date = SDF.parse(time)
        val hours = date.getHours
        var hour = ""
        if (hours < 10) {
          hour = "".concat("0").concat(hours.toString)
        } else {
          hour = "".concat(hours.toString)
        }
        val day: String = SDF2.format(date)

        val ip = splits(1)
        val proxyIP = splits(2)
        val parsedIp = parseIp(ip, ip2regionDbPath)
        val province = parsedIp._1
        val city = parsedIp._2
        val isp = parsedIp._3

        val responsetime = splits(3).toLong
        val referer = splits(4)
        val method = splits(5)

        val url = splits(6)
        val urlSplits = url.split("://")
        val protocol = urlSplits(0)
        val urlSplits2 = urlSplits(1).split("/")
        val domain = urlSplits2(0)
        val path = urlSplits2(1)

        val httpcode = splits(7)
        val requestsize = splits(8).toLong
        val responsesizeStr = splits(9)
        var responsesize = 0L
        //脏数据处理，丢掉
        if (responsesizeStr.contains("脏数据")) {
          throw new ClassCastException("responsesize脏数据")
        }
        else {
          responsesize = responsesizeStr.toLong
        }

        val cacheState = splits(10)
        val ua = splits(11)
        val fileType = splits(12)

        //UserLog(time,hour,ip,proxyIP,responsetime,referer,method,url,httpcode,requestsize,responsesize,cacheState,ua,fileType,province,city,isp,protocol,domain,path).toString
        //day作为分区字段没有存储到str,以hdfs文件目录保存
        val str = time + "\t" + hour + "\t" + ip + "\t" + proxyIP + "\t" + responsetime + "\t" + referer + "\t" + method + "\t" + url + "\t" + httpcode + "\t" + requestsize + "\t" + responsesize + "\t" + cacheState + "\t" + ua + "\t" + fileType + "\t" + province + "\t" + city + "\t" + isp + "\t" + protocol + "\t" + domain + "\t" + path
        (day, str)
      } catch {
        case e: ClassCastException => {
          log.warn(e.getMessage)
          dirtyCount.add(1L)
          null
        }
      } finally {
        totalCount.add(1L)
      }
    }
    )
      .filter(_ != null)

    //persist logRDD后面会多次使用，且不persist会导致累加器重复计算
    logRDD.persist(StorageLevel.MEMORY_ONLY_SER)

    //获得day的Array，以便安装day=xxx分区目录存储
    val dayArray: Array[String] = logRDD.map(_._1).distinct().collect()
    dayArray.map(day => logRDD
      //过滤出当前day=xxx的数据
      .filter(_._1 == day)
      .map(_._2)
      .coalesce(1)
      .saveAsTextFile(output + "day=" + day))

    println("dirtyCount: " + dirtyCount.value)
    println("totalCount: " + totalCount.value)
    //将etl totalCount等写入mysql
    writeCnt2mysql(totalCount.value,dirtyCount.value)

    sc.stop()
  }

  case class UserLog( time:String,hour:String,ip:String,proxyIP:String,responseTime:Long,referer:String,method:String,url:String,
                      httpCode:String,requestSize:Long,responsesize:Long,cacheState:String,ua:String,fileType:String,province:String,city:String,isp:String,protocol:String,domain:String,
                      path:String)

  def parseIp(ip:String,ip2regionDbPath:String)  ={
    //995|中国|0|上海|上海市|电信|125682
    val ipMessage = IPUtil.parseIP(ip, ip2regionDbPath)
    val splits = ipMessage.split("\\|")
    //province,city,isp
    (splits(3),splits(4),splits(5))
  }

  def writeCnt2mysql(totalCount:Long,dirtyCount:Long): Unit ={
    var  con : Connection = null;
    var pstmt:PreparedStatement = null
    try {

      con = MySQLUtils.getConnection()
      pstmt = con.prepareStatement("insert  into etl_info (total_cnt,nomal_cnt,error_cnt) values (?,?,?)")
      pstmt.setLong(1,totalCount)
      pstmt.setLong(2,totalCount-dirtyCount)
      pstmt.setLong(3,dirtyCount)

      pstmt.executeUpdate

    } catch {
      case exception: Exception => println(exception)
    } finally {
      if(null != pstmt) pstmt.close()
      MySQLUtils.closeConnection(con)

    }
  }
}

```

**ETL代码  IPUtil**

``` javascript
package utils;

import org.lionsoul.ip2region.DataBlock;
import org.lionsoul.ip2region.DbConfig;
import org.lionsoul.ip2region.DbSearcher;
import org.lionsoul.ip2region.Util;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.lang.reflect.Method;

public class IPUtil {
    private static Logger logger = LoggerFactory.getLogger(IPUtil.class);

    public static String parseIP(String ip, String path) {

        if (null == path) {
            System.out.println("| Usage: java -jar ip2region-{version}.jar [ip2region db file]");
            System.exit(0);
//            return ;
        }

        File file = new File(path);
        if (file.exists() == false) {
            System.out.println("Error: Invalid ip2region.db file");
            System.exit(0);
//            return;
        }

        int algorithm = DbSearcher.BTREE_ALGORITHM;
        //算法可以传参确定，这里写死
        String algoName = "B-tree";
        if (null != algoName) {
            if (algoName.equalsIgnoreCase("binary")) {
                algoName = "Binary";
                algorithm = DbSearcher.BINARY_ALGORITHM;
            } else if (algoName.equalsIgnoreCase("memory")) {
                algoName = "Memory";
                algorithm = DbSearcher.MEMORY_ALGORITYM;
            }
        }

        DataBlock dataBlock = null;

        try {
            DbConfig config = new DbConfig();
            DbSearcher searcher = new DbSearcher(config, path);


            //define the method
            Method method = null;
            switch (algorithm) {
                case DbSearcher.BTREE_ALGORITHM:
                    method = searcher.getClass().getMethod("btreeSearch", String.class);
                    break;
                case DbSearcher.BINARY_ALGORITHM:
                    method = searcher.getClass().getMethod("binarySearch", String.class);
                    break;
                case DbSearcher.MEMORY_ALGORITYM:
                    method = searcher.getClass().getMethod("memorySearch", String.class);
                    break;
            }



            if (Util.isIpAddress(ip) == false) {
                logger.info("Error: Invalid ip address");
                logger.info("Invalid ip : " + ip);
            }


            dataBlock = (DataBlock) method.invoke(searcher, ip);

            logger.info("dataBlock: " + dataBlock.toString());
//            System.out.println(dataBlock.toString());

            searcher.close();


        } catch (Exception e) {
            e.printStackTrace();
        }


        return dataBlock.toString();
    }




}

```

**ETL代码 MySQLUtils** 

``` javascript
package utils

import java.sql.{Connection, DriverManager}


/**
 *
 * 在数据库操作中，获取connection这是一个非常重量级的操作 很耗费性能
 * @author jojo
 * @desc MySQL工具类
 **/
object MySQLUtils {

  def getConnection() = {
    Class.forName("com.mysql.jdbc.Driver")
    DriverManager.getConnection("jdbc:mysql://bigdata:3306/bigdata?user=root&useUnicode=true&characterEncoding=utf8&password=hadoop")
  }

  def closeConnection(connection:Connection): Unit = {
    if(null != connection) connection.close()
  }


}

```
**spark提交任务脚本 logETL.sh**

``` javascript
#!/bin/bash

#        sh logETL.sh  
#---------------------------------------------
#FileName:              logET.sh
#Version:               1.0
#Date:                  2020-03-15
#Author:                jojo
#Description:           example of shell script
#Notes:                 project ....
#               2020-07-01 脚本开发
#               2020-07-10 脚本bug修复，具体bug为.....
#---------------------------------------------
spark-submit \
--name logetl \
--class etl.LogETLRDD \
--master yarn \
--deploy-mode client \
--executor-memory 2G \
--num-executors 1 \
--jars /root/jars/ip2region-1.7.2.jar \
/root/job/spark-project-1.0.jar \
file:///root/data/log.txt \
hdfs://bigdata:9000/data/ \
/root/jars/ip2region.db
```

4）	创建一个Hive表，并把数据加载到Hive表（分区表）

``` javascript
drop table if exists parsed_log;
create external table parsed_log (
`time` String
,hour String
,ip String
,proxyIP String
,responseTime Int
,referer String
,method String
,url String
,httpCode String
,requestSize Int
,responsesize bigInt
,cacheState String
,ua String
,fileType String
,province String
,city String
,isp String
,protocol String
,domain String
,path String)
partitioned by (day String)
row format delimited fields terminated by '\t'
stored as parquet;
LOAD DATA INPATH '/data/day=20150609' overwrite into table parsed_log partition(day='20150609');
SELECT * from parsed_log limit 10;
```

5）统计分析：https://help.aliyun.com/document_detail/27141.htm?spm=a2c4g.11186623.0.0.243d98d8dc5v2f#task-261642
前提：SQL中是带day分区的
PV/UV	

``` javascript
SELECT 
domain
,count(*) pv
from parsed_log
where day='20150609'
GROUP BY domain

SELECT 
domain
,count(distinct(user_id)) uv
from parsed_log
where day='20150609'
GROUP BY domain
```

省份、运营商	基于流量  Top10 

``` javascript
SELECT 
province
,sum(responsesize) flow_sum
from parsed_log
where day='20150609'
GROUP BY province
ORDER BY flow_sum DESC limit 10

SELECT 
isp
,sum(responsesize) flow_sum
from parsed_log
where day='20150609'
GROUP BY isp
ORDER BY flow_sum DESC limit 10
```

6） 将统计结果输出到MyQL表中

``` javascript
drop table if exists pv ;
create external table pv(
domain String
,pv Int
,day String)
row format delimited fields terminated by '\t';

INSERT OVERWRITE table pv 
SELECT 
domain
,count(*) pv
,day
from parsed_log
where day='20150609'
GROUP BY domain,day;

```

sqoop

``` javascript
sqoop export \
--connect jdbc:mysql://bigdata:3306/bigdata \
--username root \
--password hadoop \
--table pv \
 --fields-terminated-by '\t' \
 --export-dir  /user/hive/warehouse/bigdata.db/pv
```

封装

``` javascript
#!/bin/bash

#        sh dalibao.sh db 
#---------------------------------------------
#FileName:		standard.sh
#Version:		1.0
#Date:			2020-03-15
#Author:		jojo
#Description:		example of shell script
#Notes:			project ....
#		2020-07-01 脚本开发
#		2020-07-10 脚本bug修复，具体bug为.....
#---------------------------------------------
exe_day=20150610
exe_day=`date -d "1 day ago $exe_day" +%Y%m%d`
echo "exe_day="$exe_day

hive -e "drop table if exists bigdata.pv;"
sql01="create  table bigdata.pv( domain String ,pv Int ,day String) row format delimited fields terminated by '\t';"

echo "sql01="$sql01
hive -e "$sql01"

if [[ $? -ne 0 ]];then
    echo "ERROR：$?"
    exit 1
fi

sql02="INSERT OVERWRITE table bigdata.pv SELECT domain ,count(*) pv ,day from bigdata.parsed_log where day=$exe_day GROUP BY domain,day;"

echo "sql02="$sql02
hive -e "$sql02"

if [[ $? -ne 0 ]];then
    echo "ERROR：$?"
    exit 1
fi

#sqoop从hive 导出到mysql

sqoop export \
--connect jdbc:mysql://bigdata:3306/bigdata \
--username root \
--password hadoop \
--table pv \
 --fields-terminated-by '\t' \
 --export-dir  /user/hive/warehouse/bigdata.db/pv


```

7）请在一个机器上以Multi Executor Server部署Azkaban，并将作业以依赖的方式调度起来
**distributed multiple-executor mode**
``` javascript
1.编译
github下载release版本
	解压unzip
		./gradlew build installDist -x test
		建议
			提前下载
				https://services.gradle.org/distributions/gradle-4.6-all.zip
					gradle-4.6-all.zip放到gradle/wrapper下再编译
			注释掉$AZKABAN_HOME/gradle/wrapper/gradle-wrapper.properties的distributionUrl=https\://services.gradle.org/distributions/gradle-4.6-all.zip
				加上distributionUrl=gradle-4.6-all.zip
				
2.mysql环境
Set up Mysql
	CREATE DATABASE azkaban;
	CREATE USER 'azkaban'@'%' IDENTIFIED BY 'azkaban';
	GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to 'azkaban'@'%' WITH GRANT OPTION;
	flush privileges;
	
	vim /etc/my.cnf
	[mysqld]
	...
	max_allowed_packet=1024M

	service mysqld restart
	
Create the Azkaban Tables
	mysql -uroot -phadoop -Dazkaban</root/source/azkaban-3.88.0/azkaban-db/build/distributions/azkaban-db-0.1.0-SNAPSHOT/create-all-sql-0.1.0-SNAPSHOT.sql

3.Installing Azkaban Executor Server
	在azkaban-3.88.0/azkaban-exec-server/build/install/azkaban-exec-server下
	
	vim conf/azkaban.properties
	
		default.timezone.id=Asia/Shanghai
		mysql.port=3306
		mysql.host=localhost
		mysql.database=azkaban
		mysql.user=azkaban
		mysql.password=azkaban
	
	./bin/start-exec.sh
	
	activate the executor
		cd azkaban-exec-server/build/install/azkaban-exec-server
		curl -G "localhost:$(<./executor.port)/executor?action=activate" && echo
4.Installing Azkaban Web Server
	cd azkaban/azkaban-web-server/build/install/azkaban-web-server
	
	vim conf/azkaban.properties
	
		default.timezone.id=Asia/Shanghai
		mysql.port=3306
		mysql.host=localhost
		mysql.database=azkaban
		mysql.user=azkaban
		mysql.password=azkaban
		
	启动
		cd azkaban-web-server/build/install/azkaban-web-server
		./bin/start-web.sh
		
	登陆
		http://bigdata:8081/
		azkaban
		azkaban
	
注意 ：
	以上方式是快速启动
	分布式多executor，应该从azkaban-3.88.0/azkaban-exec-server/build/distributions和azkaban-3.88.0/azkaban-web-server/build/distributions获取tar 解压部署到相应节点
```

-----------------------------------------------------------------------------------------------------	
	

## 春劫大礼包之二：

请使用RDD、DF/DS API功能实现最大连续问题（参见元旦大礼包的SQL题，对比不同实现方式的结果是否一致）

user     times   start_date   end_date
pk       4       2021-08-01  2021-08-04
ruoze    3       2021-07-30  2021-08-01

rdd

``` javascript
import org.apache.spark.{SparkConf, SparkContext}

import java.text.SimpleDateFormat
import java.util.Calendar

object ContinuousDayV2 {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf()
      .setAppName(this.getClass.getCanonicalName)
      .setMaster("local[2]")
    val  sc = new SparkContext(sparkConf)
    //rdd求连续最大天数
    //常规思路： 按照时间dt排序，rank开窗rk，dt - rk = diff ,在根据name,diff分组，使用count(*)的得到无一天中断的连续次数cnt
    val format = new SimpleDateFormat("yyyyMMdd")
    val calendar = Calendar.getInstance()
    val path = "file:///D:\\code\\bigdata2021\\data\\input\\rdd\\continuous_day.txt"
    //pk,20210801
    sc.textFile(path)
      .map(line => {
        val splits = line.split(",")
        (splits(0),splits(1))})
      .groupByKey()
      .flatMapValues( iter => {
        //日期去重排序
        val dateList: List[String] = iter.toList.toSet.toList.sorted
        var rank = 0
        dateList.map(date_str => {
          //分组内每组依次减去rank,生成(date_str,date_diff)
          calendar.setTime(format.parse(date_str))
          calendar.add(Calendar.DAY_OF_YEAR,-rank)
          rank += 1
          (date_str,format.format(calendar.getTime))
        })
      })
      //将数据改为((name,date_diff),date_str)
      .map(x => ((x._1,x._2._2),x._2._1))
      .groupByKey()
      .mapValues(iter => (iter.size,iter.min,iter.max))   //((ruoze,20210730),(3,20210801,20210804))     ((pk,20210801),4)     ((ruoze,20210801),1)
      .map(x => (x._1._1,(x._2._1,x._2._2,x._2._3)))
//    (ruoze,(3,20210730,20210801))
//    (ruoze,(1,20210804,20210804))
//    (pk,(3,20210806,20210808))
//    (pk,(2,20210811,20210812))
//    (pk,(4,20210801,20210804))
      .groupByKey()                     //按name分组，取最大的
      .mapValues(tup3 => tup3.toList.sortBy(x => -x._1).max)
      .foreach(println(_))

    sc.stop()

  }




}

```
df api

``` javascript
import org.apache.spark.sql.{DataFrame, Row, SparkSession}

import org.apache.spark.sql.expressions.Window
import java.text.SimpleDateFormat
import java.util.Calendar

object ContinuousDayV3 {
  def main(args: Array[String]): Unit = {
    import org.apache.spark.sql.functions._

    val spark = SparkSession.builder()
      .appName(this.getClass.getCanonicalName)
      .master("local[2]")
      .getOrCreate()
    //rdd求连续最大天数
    //常规思路： 按照时间dt排序，rank开窗rk，dt - rk = diff ,在根据name,diff分组，使用count(*)的得到无一天中断的连续次数cnt
    val format = new SimpleDateFormat("yyyyMMdd")
    val calendar = Calendar.getInstance()
    val path = "file:///D:\\code\\bigdata2021\\data\\input\\rdd\\continuous_day.txt"
    //pk,20210801

    val df: DataFrame = spark.read.format("csv").option("sep", ",").load(path)
      .withColumnRenamed("_c0","name")
      .withColumnRenamed("_c1","day")

    val df2 = df.withColumn("format_day", from_unixtime(unix_timestamp(df("day"), "yyyyMMdd"),"yyyy-MM-dd"))
      .withColumn("rank",row_number().over(Window.partitionBy("name").orderBy("day")))

    val df3: DataFrame = df2.withColumn("date_rank", date_sub(df2("format_day"), df2("rank")))
      .groupBy("date_rank", "name")
      .agg(count("*").as("cnt"), min("day").as("start_day"), max("day").as("end_day"))

    val df4 = df3.sort(df3("name"), df3("cnt").desc)
 

    df4.printSchema()
    df4.show()

   spark.stop()

  }




}

```



-----------------------------------------------------------------------------------------------------

### 春劫大礼包之三：

数据说明：
  下载地址：http://files.grouplens.org/datasets/movielens/ml-25m.zip
  movies.csv
    movieId,title,genres
    电影id，电影名称，电影所属分类
  ratings.csv
    userId,movieId,rating,timestamp
    用户id，电影id，评分，时间戳
需求：使用Spark SQL的SQL以及DF(DS)API完成如下的统计分析并将结果写入到MySQL中
  查找电影评分个数超过5000,且平均评分较高的前十部电影名称及其对应的平均评分
  查找每个电影类别及其对应的平均评分
  查找被评分次数较多的前十部电影
  
  

``` javascript

import org.apache.spark.sql.types.{DoubleType}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.junit.{After, Before, Test}
import org.apache.spark.sql.expressions.Window

class MovieTest {

  import org.apache.spark.sql.functions._
  var spark : SparkSession = _
  var df01: DataFrame = _
  var df02: DataFrame = _

  @Before
  def setup: Unit ={
     spark = SparkSession.builder()
      .master("local[2]")
      .appName(this.getClass.getSimpleName)
       .getOrCreate()
//    import spark.implicits._
    val path01 = "file:///D:\\2021\\homework\\ml-25m\\movies.csv"
    val path02 = "file:///D:\\2021\\homework\\ml-25m\\ratings.csv"

    df01 = spark.read.format("csv").option("header","true").load(path01).toDF( "movieId","title","genres")
    df02 = spark.read.format("csv").option("header","true").load(path02).toDF( "userId","movieId","rating","timestamp")
    df02 = df02.select(df02("userId"),df02("movieId"),df02("rating").cast(DoubleType),df02("timestamp"))


    df01.createOrReplaceTempView("movies")
    df02.createOrReplaceTempView("ratings")
  }

  @Test
  def test01: Unit = {
spark.sql(
  //查找电影评分个数超过5000,且平均评分较高的前十部电影名称及其对应的平均评分
  """
  select
    |m.title
    |,b.avg_rating
    |from
    |(
    |    select
    |        a.movieId
    |        ,a.cnt
    |        ,a.avg_rating
    |        ,row_number() over( order by a.avg_rating desc) rank
    |    from
    |        (
    |        select
    |            movieId
    |            ,count(*) cnt
    |            ,avg(rating) avg_rating
    |        from ratings
    |            group by movieId
    |        ) a
    |    where a.cnt>5000
    |) b
    |join movies m on b.movieId=m.movieId
    |where b.rank<=10
    |
    |""".stripMargin).show(false)
  }

  @Test
  def test02: Unit ={
    //查找每个电影类别及其对应的平均评分
    //Action|  Adventure
    spark.sql(
      """
        select
        |    a.genre
        |    ,avg(a.rating) avg_rating
        |from
        |    (
        |    select
        |        m.genre
        |        ,r.rating
        |    from
        |    ratings r join
        |        (
        |        select
        |        movieId
        |        ,trim(genre_tmp) genre
        |        from movies
        |        lateral view explode(split(genres,"\\|")) tmp as genre_tmp
        |        ) m
        |    on r.movieId=m.movieId
        |    )a
        |group by a.genre
        |""".stripMargin).show(false)
  }
  @Test
  def test03: Unit ={
    //查找被评分次数较多的前十部电影
  spark.sql(
    """
     select
      |    m.title
      |    ,r.cnt
      |from
      |movies m
      |join
      |    (
      |     select
      |         b.movieId
      |         ,b.cnt
      |     from
      |        (
      |         select
      |             a.movieId
      |             ,a.cnt
      |             ,row_number() over(order by a.cnt desc) rank
      |         from
      |            (
      |                select
      |                    movieId
      |                    ,count(*) cnt
      |                from
      |                ratings
      |                group by movieId
      |            ) a
      |        ) b
      |     where b.rank<=10
      |    ) r
      |on r.movieId=m.movieId
      |""".stripMargin).show(false)
  }
@Test
  def test04: Unit ={
  //查找电影评分个数超过5000,且平均评分较高的前十部电影名称及其对应的平均评分
  val df03: DataFrame = df02.groupBy("movieId")
    .agg(count("*").as("cnt"), avg("rating").as("avg_rating"))
    .filter("cnt>5000")
    .toDF()
  df03
    .withColumn("rank", row_number().over(Window.orderBy(df03("avg_rating").desc)))
    .filter("rank<=10")
    .join(df01, df01("movieId") === df03("movieId"))
    .select(df01("title"),df03("avg_rating")).show()
  }
@Test
  def test05: Unit ={
  // 查找每个电影类别及其对应的平均评分
  val df03 = df01.withColumn("genre_tmp", explode(split(df01("genres"), "\\|")))
  val df04 = df03.select(df03("movieId"), trim(df03("genre_tmp")).as("genre"))
  df04
    .join(df02,df04("movieId")===df02("movieId"))
    .select(df04("genre"),df02("rating"))
    .groupBy("genre")
    .agg(avg("rating").as("avg_rating"))
    .select("genre","avg_rating")
      .show()
  }
  @Test
  def test06: Unit ={
    //查找被评分次数较多的前十部电影
    val df03 = df02.groupBy("movieId")
      .agg(count("*").as("cnt"))
    val df04 = df03
      .withColumn("rank", row_number().over(Window.orderBy(df03("cnt").desc)))
      .filter("rank<=10")
    df04
      .join(df01,df01("movieId")===df04("movieId"))
      .select(df01("title"),df04("cnt"))
      .show(false)

  }

  @After
  def cleanuup: Unit ={
    spark.stop()
  }


}

```





20220228-20220206
Q1：准备数据：至少要准备2份（一天一份日志数据）   用DF/DS实现
ruozedata.com    a\tb      gzip
google.com       b,c         bzip2
baidu.com        a$$$c    不压缩
ruozedata.com/ruozedata.com_20210101.gz
ruozedata.com/ruozedata.com_20210102.gz

google.com/google.com_210101.bz2
google.com/google.com_210102.bz2

baidu.com/baidu.com_0101.log
baidu.com/baidu.com_0102.log

输出文件命名要求：以域名作为输出文件夹，以域名_时间格式（也需要自定义）为文件名，没个域名只有一个输出文件

ruozedata.com/ruozedata.com_20210101.gz
ruozedata.com/ruozedata.com_20210102.gz

google.com/google.com_210101.bz2
google.com/google.com_210102.bz2

baidu.com/baidu.com_0101.log
baidu.com/baidu.com_0102.log


``` javascript
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.io.compress.{BZip2Codec, GzipCodec}
import org.apache.hadoop.mapred.lib.MultipleTextOutputFormat
import org.apache.hadoop.mapred.{FileOutputFormat, JobConf, RecordWriter}
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.storage.StorageLevel
import utils.FileUtils

import java.text.SimpleDateFormat
import java.util.{Date, Locale}

object AccessApp {
  def main(args: Array[String]): Unit = {
    System.setProperty("HADOOP_USER_NAME","root")

    val spark = SparkSession.builder()
      .appName(this.getClass.getName)
      .master("local[2]")
      .getOrCreate()
  import spark.implicits._
//    val path = "file:///D:\\code\\bigdata2021\\data\\input\\access.json"
      val path = "file:///D:\\code\\bigdata2021\\data\\input\\access2.json"

    val df: DataFrame = spark.read
      .format("json")
      .option("inferschema", "true")
      .load(path)
    df.persist(StorageLevel.MEMORY_ONLY)
  //{"appId":"google.com","duration":"5336","ip":"222.37.240.28","platform":"Symbian","time":"2019-09-11 22:35:34","traffic":"~~~","user":"ruozedata20","version":"10.0.2"}

    val rzdf = df
      .filter('appId === "ruozedata.com")
      .select('appId,'time,'duration,'ip)

    val ggdf = df
      .filter('appId === "google.com")
      .select('appId,'time,'traffic,'ip)

    val bddf = df
      .filter('appId === "baidu.com")
      .select('appId,'time,'user,'ip)

    val out= "/data"
//    FileUtils.deleteOutput(spark.sparkContext.hadoopConfiguration,out)


    rzdf.rdd.map(x => (x.getAs[String]("appId") + "_" +x.getAs[String]("time"),x.mkString("\t")))
      .coalesce(1)
    .saveAsHadoopFile(out,classOf[String],classOf[String],classOf[CustomOutputFormat],classOf[GzipCodec])

    ggdf.rdd.map(x => (x.getAs[String]("appId") + "_" +x.getAs[String]("time"),x.mkString(",")))
      .coalesce(1)
      .saveAsHadoopFile(out,classOf[String],classOf[String],classOf[CustomOutputFormat],classOf[BZip2Codec])

    bddf.rdd.map(x => (x.getAs[String]("appId") + "_" +x.getAs[String]("time"),x.mkString("$$$")))
      .coalesce(1)
      .saveAsHadoopFile(out,classOf[String],classOf[String],classOf[CustomOutputFormat])

//    rzdf.printSchema()
//    rzdf.show()

    spark.close()
  }



  class CustomOutputFormat extends MultipleTextOutputFormat[String, String] {
    val SDF = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss", Locale.ENGLISH)
    val SDF2 = new SimpleDateFormat("yyyyMMdd", Locale.ENGLISH)
    val SDF3 = new SimpleDateFormat("yyMMdd", Locale.ENGLISH)
    val SDF4 = new SimpleDateFormat("MMdd", Locale.ENGLISH)

    /**
     * 重写generateFileNameForKeyValue方法，该方法是负责自定义生成文件的文件名
     */
    override def generateFileNameForKeyValue(key: String, value: String, name: String): String = {
      //这里的key和value指的就是要写入文件的rdd对，再此，我定义文件名以key.txt来命名，当然也可以根据其他的需求来进行生成文件名
      val splits = key.split("_")
      val domain = splits(0)
      val dateStr = splits(1)
      val date: Date = SDF.parse(dateStr)
      var formattedDate = ""
      var suffix = ""
      if(domain.equals("ruozedata.com")){
        formattedDate = SDF2.format(date)
      }else if(domain.equals("google.com")){
        formattedDate = SDF3.format(date)
      }else if(domain.equals("baidu.com")){
        formattedDate = SDF4.format(date)
        suffix = ".log"
      }

      val fileName = domain + "/" + domain + formattedDate + suffix
      fileName
    }

    /**
     * 因为saveAsHadoopFile是以key,value的形式保存文件，写入文件之后的内容也是，按照key value的形式写入，
     * k,v之间用空格隔开，这里我只需要写入value的值，不需要将key的值写入到文件中个，所以需要重写该方法。
     * 让输入到文件中的key为空即可，当然也可以进行领过的变通，也可以重写generateActuralValue(key:Any,value:Any),根据自己的需求来实现
     */
    override def generateActualKey(key: String, value: String): String = {
      null
    }

    //对生成的value进行转换为字符串，当然源码中默认也是直接返回value值，如果对value没有特殊处理的话，不需要重写该方法
    //  override def generateAcutalValue(key: Any, value: Any): String = {
    //    return value.asInstanceOf[String]
    //  }

    /**
     * 该方法使用来检查我们输出的文件目录是否存在，源码中，是这样判断的，如果写入的父目录已经存在的话，则抛出异常
     * 在这里我们重写这个方法，修改文件目录的判断方式，如果传入的文件写入目录已存在的话，直接将其设置为输出目录即可，
     * 不会抛出异常
     */
    override def checkOutputSpecs(ignored: FileSystem, job: JobConf): Unit = {
      var outDir: Path = FileOutputFormat.getOutputPath(job)
      if (outDir != null) {
        //注意下面的这两句，如果说你要是写入文件的路径是hdfs的话，下面的两句不要写，或是注释掉，
        // 它俩的作用是标准化文件输出目录，根据我的理解是，他们是标准化本地路径，写入本地的话，可以加上，本地路径记得要用file:///开头，比如file:///E:/a.txt
        //val fs: FileSystem = ignored
        //outDir = fs.makeQualified(outDir)
        FileOutputFormat.setOutputPath(job, outDir)
      }
    }


  }

}

```

